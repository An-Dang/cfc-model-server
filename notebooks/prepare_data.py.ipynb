{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting datat from xnli and rte3, cleaning and combining them in on big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import xml\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"../data/\"\n",
    "rte3_path = data_path + \"rte3/\"\n",
    "xnli_path = data_path + \"xnli/\"\n",
    "mnli_path = data_path + \"mnli/\"\n",
    "\n",
    "preprocessed_path = data_path + \"preprocessed/\"\n",
    "rte3_preprocessed_path = os.path.join(preprocessed_path, \"rte3.tsv\")\n",
    "xnli_preprocessed_path = os.path.join(preprocessed_path, \"xnli.tsv\")\n",
    "mnli_preprocessed_path = os.path.join(preprocessed_path, \"mnli.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process RTE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                           sentenceA  \\\n",
       " 0  Hallo Folgendes Problem, der CPU ist bei 100%....   \n",
       " 1  Hallo, habe mir ein Asus P5Q geholt und am Anf...   \n",
       " 2  hallo liebe mods, ich schreibe diesen beitrag ...   \n",
       " 3  Hallo, ich habe Probleme mit dem Trojaner '' T...   \n",
       " 4  Hallo, ich habe Probleme mit dem Trojaner '' T...   \n",
       " \n",
       "                                            sentenceB       label  \n",
       " 0  Der Computer ist durch Malware verseucht, der ...  entailment  \n",
       " 1  Die USB-Anschlüsse funktionieren bei dem neuen...  entailment  \n",
       " 2  Beim Aufrufen des p5q deluxe Sammelthreads bek...  entailment  \n",
       " 3            Ein Trojaner kann nicht entfernt werden  entailment  \n",
       " 4  Mein Virenprogramm AntiVir meldet immer wieder...  entailment  ,\n",
       " 3014)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=[\"sentenceA\", \"sentenceB\", \"label\"]\n",
    "dataframe = pd.DataFrame(columns=columns)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "devel_file = os.path.join(rte3_path, \"german_social_media_DEVEL.xml\")\n",
    "test_file = os.path.join(rte3_path, \"german_social_media_TEST.xml\")\n",
    "\n",
    "\n",
    "def parse_and_collect_rte(filepath, rte3_dataframe):\n",
    "    dom = ET.parse(filepath)\n",
    "    for pair in dom.iter(\"pair\"):\n",
    "        if pair.tag == \"pair\":\n",
    "            label = pair.attrib[\"entailment\"]\n",
    "            sentenceA = pair[0].text\n",
    "            sentenceB = pair[1].text\n",
    "\n",
    "            rte3_dataframe = rte3_dataframe.append({\"sentenceA\": sentenceA, \"sentenceB\": sentenceB, \"label\": label.lower().replace(\"nonentailment\",\"contradiction\")}, ignore_index=True)\n",
    "    return rte3_dataframe\n",
    "\n",
    "dataframe = parse_and_collect_rte(devel_file, dataframe)\n",
    "dataframe = parse_and_collect_rte(test_file, dataframe)\n",
    "dataframe.to_csv(os.path.join(data_path + \"preprocessed\", \"rte3.tsv\"),sep=\"\\t\", encoding=\"utf8\", index=False)\n",
    "\n",
    "dataframe.head(), len(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_collect_nli(filepath, xnli_dataframe, nrows=30000):\n",
    "    xnli_data = pd.read_csv(filepath, encoding=\"utf8\", sep='\\t', error_bad_lines=False, low_memory=True, dtype='unicode', nrows=nrows)\n",
    "    for idx, row in tqdm(xnli_data.iterrows(), total=len(xnli_data)):\n",
    "        language_key =\"language\"\n",
    "        if (language_key in row and row[language_key] == \"de\") or language_key not in row:\n",
    "            xnli_dataframe = xnli_dataframe.append({\"sentenceA\": row[\"sentence1\"], \"sentenceB\": row[\"sentence2\"], \"label\": row[\"gold_label\"].lower()}, ignore_index=True)\n",
    "    return xnli_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 30000/30000 [00:06<00:00, 4301.08it/s]\n",
      "100%|██████████████████████████████████| 30000/30000 [00:12<00:00, 2352.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                           sentenceA  \\\n",
       " 0            und er hat gesagt, Mama ich bin daheim.   \n",
       " 1            und er hat gesagt, Mama ich bin daheim.   \n",
       " 2            und er hat gesagt, Mama ich bin daheim.   \n",
       " 3  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
       " 4  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
       " \n",
       "                                            sentenceB          label  \n",
       " 0  Er rief seine Mutter an, sobald er aus dem Sch...        neutral  \n",
       " 1                                Er sagte kein Wort.  contradiction  \n",
       " 2  Er sagte seiner Mutter, er sei nach Hause geko...     entailment  \n",
       " 3  Ich war noch nie in Washington, deshalb habe i...        neutral  \n",
       " 4  Ich wusste genau, was ich tun musste, als ich ...  contradiction  ,\n",
       " 7500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=[\"sentenceA\", \"sentenceB\", \"label\"]\n",
    "dataframe = pd.DataFrame(columns=columns)\n",
    "\n",
    "devel_file = os.path.join(xnli_path, \"xnli.dev.tsv\")\n",
    "test_file = os.path.join(xnli_path, \"xnli.test.tsv\")\n",
    "    \n",
    "dataframe = parse_and_collect_nli(devel_file, dataframe)\n",
    "dataframe = parse_and_collect_nli(test_file, dataframe)\n",
    "dataframe.to_csv(os.path.join(data_path + \"preprocessed\", \"xnli.tsv\"), sep=\"\\t\", encoding=\"utf8\", index=False)\n",
    "\n",
    "dataframe.head(), len(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combin both and build sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_dataframe = pd.read_csv(rte3_preprocessed_path, encoding=\"utf8\", sep='\\t')\n",
    "rte3_dataframe = pd.read_csv(xnli_preprocessed_path, encoding=\"utf8\", sep='\\t')\n",
    "\n",
    "all_data = pd.concat([xnli_dataframe, rte3_dataframe])\n",
    "all_data = all_data.sample(frac=1)\n",
    "\n",
    "train_size=0.8\n",
    "train_set = all_data.sample(frac=train_size, random_state=0)\n",
    "test_set = all_data.drop(train_set.index)\n",
    "\n",
    "train_set.to_csv(os.path.join(preprocessed_path, \"train.tsv\"), sep=\"\\t\", encoding=\"utf8\", index=False)\n",
    "test_set.to_csv(os.path.join(preprocessed_path, \"test.tsv\"), sep=\"\\t\", encoding=\"utf8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat, translate and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9897/9897 [00:22<00:00, 449.70it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:28<00:00, 356.14it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:22<00:00, 435.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(columns=columns)\n",
    "\n",
    "dataframe = parse_and_collect_nli(os.path.join(mnli_path, \"multinli_1.0_dev_matched.txt\"), dataframe)\n",
    "dataframe = parse_and_collect_nli(os.path.join(mnli_path, \"multinli_1.0_dev_mismatched.txt\"), dataframe)\n",
    "dataframe.to_csv(os.path.join(preprocessed_path, \"mnli_dev.tsv\"), sep=\"\\t\", encoding=\"utf8\", index=False)\n",
    "\n",
    "dataframe = pd.DataFrame(columns=columns)\n",
    "dataframe = parse_and_collect_nli(os.path.join(mnli_path, \"multinli_1.0_train.txt\"), dataframe, nrows=10000)\n",
    "dataframe.to_csv(os.path.join(preprocessed_path, \"mnli_train.tsv\"), sep=\"\\t\", encoding=\"utf8\", index=False)\n",
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate train to german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                     | 117/10000 [00:12<16:30,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native  'Me win, me passum heap big law ... nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████▎                        | 3318/10000 [18:27<34:46,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint-Germain-des-Pr??s nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [55:44<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import six\n",
    "from tqdm import tqdm\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "# set creds env\n",
    "cwd = os.getcwd()\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"D:/Repositories/semantic-match-classifier/coronacheck-ai-9172d153d26a.json\"\n",
    "demo_text = \"This is a medical fact.\"\n",
    "\n",
    "translate_client = translate.Client()\n",
    "\n",
    "def do_translate(text):\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(\n",
    "        text, target_language=\"de\", source_language=\"en\")\n",
    "    return result['translatedText']\n",
    "\n",
    "dataframe_translated = pd.DataFrame(columns=columns)\n",
    "dataframe = pd.read_csv(os.path.join(preprocessed_path, \"mnli_train.tsv\"), sep=\"\\t\", encoding=\"utf8\")\n",
    "for idx, row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n",
    "    if not pd.isna(row[\"sentenceA\"]) and not pd.isna(row[\"sentenceB\"]):\n",
    "        sentence_a_trans = do_translate(row[\"sentenceA\"])\n",
    "        sentence_b_trans = do_translate(row[\"sentenceB\"])\n",
    "        dataframe_translated = dataframe_translated.append({\"sentenceA\": sentence_a_trans, \"sentenceB\": sentence_b_trans, \"label\": row[\"label\"]}, ignore_index=True)\n",
    "    else:\n",
    "        print(row[\"sentenceA\"], row[\"sentenceB\"])\n",
    "dataframe_translated.to_csv(os.path.join(preprocessed_path, \"mnli_train_translated.tsv\"), sep=\"\\t\", encoding=\"utf8\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
